{"cells":[{"cell_type":"markdown","metadata":{"id":"eFzWueE6d1W9"},"source":["# Rotten Tomatoes Sentiment Classification\n","Rotten Tomatoes Dataset (https://www.kaggle.com/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews).\n","\n","\n","## Section 0: Importing libraries\n","\n","Add all of the imports you will use in this analysis here.  If you choose to install any other libraries, make sure to keep that code in this section."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91331,"status":"ok","timestamp":1733280789043,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"kOk3t_KSGRlr","outputId":"18f09e04-b960-4075-ddfe-fe8b21a828b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Collecting sweetviz\n","  Downloading sweetviz-2.3.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (2.2.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.8.0)\n","Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (4.66.6)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.13.1)\n","Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.1.4)\n","Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (6.4.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.1->sweetviz) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n","Downloading sweetviz-2.3.1-py3-none-any.whl (15.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sweetviz\n","Successfully installed sweetviz-2.3.1\n","Collecting fast-langdetect\n","  Downloading fast_langdetect-0.2.2-py3-none-any.whl.metadata (5.3 kB)\n","Collecting fasttext-wheel>=0.9.2 (from fast-langdetect)\n","  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting robust-downloader>=0.0.2 (from fast-langdetect)\n","  Downloading robust_downloader-0.0.2-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from fast-langdetect) (1.26.4)\n","Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from fast-langdetect) (2.32.3)\n","Collecting pybind11>=2.2 (from fasttext-wheel>=0.9.2->fast-langdetect)\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel>=0.9.2->fast-langdetect) (75.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->fast-langdetect) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->fast-langdetect) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->fast-langdetect) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->fast-langdetect) (2024.8.30)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from robust-downloader>=0.0.2->fast-langdetect) (4.66.6)\n","Collecting colorlog (from robust-downloader>=0.0.2->fast-langdetect)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Downloading fast_langdetect-0.2.2-py3-none-any.whl (786 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.3/786.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading robust_downloader-0.0.2-py3-none-any.whl (15 kB)\n","Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: pybind11, colorlog, robust-downloader, fasttext-wheel, fast-langdetect\n","Successfully installed colorlog-6.9.0 fast-langdetect-0.2.2 fasttext-wheel-0.9.2 pybind11-2.13.6 robust-downloader-0.0.2\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=0b81463c4661fae84e8563840c4604d35e70a53e8d4b3cf9d916bbcffbec3530\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n","Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n","Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement joblin (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for joblin\u001b[0m\u001b[31m\n","\u001b[0mCollecting es-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Collecting fr-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.13.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.23.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.8.30)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n","Installing collected packages: fr-core-news-sm\n","Successfully installed fr-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]}],"source":["!pip install gdown\n","!pip install sweetviz\n","!pip install fast-langdetect\n","!pip install langdetect\n","!pip install contractions\n","!pip install spacy\n","!pip install joblin\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.util import ngrams\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","\n","# Downloading Spacy\n","!python -m spacy download es_core_news_sm\n","!python -m spacy download fr_core_news_sm\n","\n","# Downloading NLTK\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","from concurrent.futures import ProcessPoolExecutor\n","from collections import Counter\n","from IPython.display import display, HTML # Need for inline HTML display\n","from fast_langdetect import detect # for speed\n","from langdetect import LangDetectException\n","from textblob import TextBlob\n","from spacy.util import minibatch\n","from joblib import Parallel, delayed # for speed\n","from pprint import pprint\n","from bs4 import BeautifulSoup\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.metrics import accuracy_score, f1_score as sklearn_f1_score, fbeta_score, precision_recall_fscore_support\n","from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, precision_recall_curve, classification_report, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","import fasttext\n","import fasttext.util\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gdown\n","import numpy as np\n","import scipy.stats as stats\n","import sweetviz as sv\n","import string\n","import re\n","import html\n","import contractions\n","import textblob\n","import spacy\n","import pickle\n","import torch\n","import requests\n","import tabulate"]},{"cell_type":"markdown","metadata":{"id":"LlFqyWoBVWBm"},"source":["## Section 1: Getting Data\n","\n","1) Download data from [here](https://drive.google.com/file/d/1DbyBgw3riR__gdEk1y2Xpo2VbBbXn3Eh/view?usp=sharing).  Note: this may take 15-20 seconds or so.\n","\n","2) Upload the data in this Colab environment (heads up: you will lose this dataset when you restart your session)\n","\n","3) Read the dataset into this notebook"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"elapsed":11218,"status":"ok","timestamp":1733280800259,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"ippfsjIiVXNN","outputId":"7c2d020b-1bc8-486a-be27-0bd7380994b8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1DbyBgw3riR__gdEk1y2Xpo2VbBbXn3Eh\n","To: /content/tomato_reviews.csv\n","100%|██████████| 94.4M/94.4M [00:01<00:00, 72.8MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["                     id   reviewId creationDate     criticName  isTopCritic  \\\n","0  small_town_wisconsin  102711819   2022-07-22     Peter Gray        False   \n","1  small_town_wisconsin  102711545   2022-07-22   Tim Grierson         True   \n","2  small_town_wisconsin  102700937   2022-06-16  Sumner Forbes        False   \n","3  small_town_wisconsin  102699897   2022-06-14  Tara McNamara        False   \n","4  small_town_wisconsin  102698744   2022-06-10     Rob Thomas        False   \n","\n","  originalScore reviewState               publicatioName  \\\n","0           NaN       fresh                 This is Film   \n","1           NaN       fresh         Screen International   \n","2        8.5/10       fresh                  Film Threat   \n","3           3/5       fresh           Common Sense Media   \n","4           3/4       fresh  Capital Times (Madison, WI)   \n","\n","                                          reviewText scoreSentiment  \\\n","0  Small Town Wisconsin could hit some home truth...       POSITIVE   \n","1  This low-key drama has lovely interludes and s...       POSITIVE   \n","2  Small Town Wisconsin is a success in almost ev...       POSITIVE   \n","3  Just like Wayne&#44; Small Town Wisconsin has ...       POSITIVE   \n","4  It&#8217;s a movie with its heart in the right...       POSITIVE   \n","\n","                                           reviewUrl  \n","0  https://thisisfilm.com/review/small-town-wisco...  \n","1  https://www.screendaily.com/reviews/small-town...  \n","2  https://filmthreat.com/reviews/small-town-wisc...  \n","3  https://www.commonsensemedia.org/movie-reviews...  \n","4  https://captimes.com/entertainment/screens/sma...  "],"text/html":["\n","  <div id=\"df-fc51dcda-5ee0-4150-b3f3-e27f99e38fa0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>reviewId</th>\n","      <th>creationDate</th>\n","      <th>criticName</th>\n","      <th>isTopCritic</th>\n","      <th>originalScore</th>\n","      <th>reviewState</th>\n","      <th>publicatioName</th>\n","      <th>reviewText</th>\n","      <th>scoreSentiment</th>\n","      <th>reviewUrl</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>small_town_wisconsin</td>\n","      <td>102711819</td>\n","      <td>2022-07-22</td>\n","      <td>Peter Gray</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>fresh</td>\n","      <td>This is Film</td>\n","      <td>Small Town Wisconsin could hit some home truth...</td>\n","      <td>POSITIVE</td>\n","      <td>https://thisisfilm.com/review/small-town-wisco...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>small_town_wisconsin</td>\n","      <td>102711545</td>\n","      <td>2022-07-22</td>\n","      <td>Tim Grierson</td>\n","      <td>True</td>\n","      <td>NaN</td>\n","      <td>fresh</td>\n","      <td>Screen International</td>\n","      <td>This low-key drama has lovely interludes and s...</td>\n","      <td>POSITIVE</td>\n","      <td>https://www.screendaily.com/reviews/small-town...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>small_town_wisconsin</td>\n","      <td>102700937</td>\n","      <td>2022-06-16</td>\n","      <td>Sumner Forbes</td>\n","      <td>False</td>\n","      <td>8.5/10</td>\n","      <td>fresh</td>\n","      <td>Film Threat</td>\n","      <td>Small Town Wisconsin is a success in almost ev...</td>\n","      <td>POSITIVE</td>\n","      <td>https://filmthreat.com/reviews/small-town-wisc...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>small_town_wisconsin</td>\n","      <td>102699897</td>\n","      <td>2022-06-14</td>\n","      <td>Tara McNamara</td>\n","      <td>False</td>\n","      <td>3/5</td>\n","      <td>fresh</td>\n","      <td>Common Sense Media</td>\n","      <td>Just like Wayne&amp;#44; Small Town Wisconsin has ...</td>\n","      <td>POSITIVE</td>\n","      <td>https://www.commonsensemedia.org/movie-reviews...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>small_town_wisconsin</td>\n","      <td>102698744</td>\n","      <td>2022-06-10</td>\n","      <td>Rob Thomas</td>\n","      <td>False</td>\n","      <td>3/4</td>\n","      <td>fresh</td>\n","      <td>Capital Times (Madison, WI)</td>\n","      <td>It&amp;#8217;s a movie with its heart in the right...</td>\n","      <td>POSITIVE</td>\n","      <td>https://captimes.com/entertainment/screens/sma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc51dcda-5ee0-4150-b3f3-e27f99e38fa0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fc51dcda-5ee0-4150-b3f3-e27f99e38fa0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fc51dcda-5ee0-4150-b3f3-e27f99e38fa0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-64e39065-de99-4b1e-88b2-313177d78f09\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-64e39065-de99-4b1e-88b2-313177d78f09')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-64e39065-de99-4b1e-88b2-313177d78f09 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"RT_df"}},"metadata":{},"execution_count":2}],"source":["# Download ID for Rotten Tomatoes Reviews\n","download_id = '1DbyBgw3riR__gdEk1y2Xpo2VbBbXn3Eh'\n","download_url = f'https://drive.google.com/uc?id={download_id}'\n","\n","# Downloading to output path\n","output_path = 'tomato_reviews.csv'\n","gdown.download(download_url, output_path, quiet=False)\n","RT_df = pd.read_csv(output_path) # Rotten Tomatoes Review DF\n","RT_df.head()"]},{"cell_type":"code","source":["# Set the option to display the full content of the 'reviewText' column\n","pd.set_option('display.max_colwidth', None)\n","\n","# Filter rows containing the word \"arrival\" in the 'id' column (case insensitive)\n","arrival_ids = RT_df[RT_df['id'].str.contains('arrival', case=False, na=False)]\n","\n","# Show only the 'publicationName' and 'reviewText' columns\n","arrival_ids_filtered = arrival_ids[['publicatioName', 'reviewText']]\n","\n","# Pretty print the result using tabulate\n","print(arrival_ids_filtered.to_string(index=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XDK0TcKjjym","executionInfo":{"status":"ok","timestamp":1733280891498,"user_tz":480,"elapsed":891,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"302de89c-07ba-4a13-8246-b8549377afe8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["                           publicatioName                                                                                                                                                                                                                                                                 reviewText\n","                      The Virginian-Pilot                                                                                                                                                      E.T. would find it false propaganda, but for fans of the genre, it's standard sci-fi with all the needed ingredients.\n","                      Gone With The Twins                                                                                                                    The film starts to fall apart at the introduction of the aliens themselves, which suffer from dated special effects and an immediate aura of disbelief.\n","                               Cinegarage                                                                                                                                                          Leaves us with hope... One of the best sci-fi films of recent times without gunfights. [Full review in Spanish]\\n\n","                      Winnipeg Free Press                           Melancholy, mysterious and measured as it tracks the struggle to communicate with these alien creatures, Arrival is really about language and how it functions, how it shapes us and sometimes limits us, and how it can connect or separate us.\n","                         Washington Blade                                                                                                                                                                                                                         Amy Adams’ transcendent portrayal of a linguist...\n","                       Keith & the Movies I appreciated the intelligent science fiction&#44; but also how the film steps beyond genre&#46; It turned out to be far more intimate and thought-provoking than I ever expected&#46; And all of that on top of the superb visuals&#44; art direction&#44; and score&#46;\n","                 Every Movie Has a Lesson                                                          The trippy events unfolding out of Heisserer&#8217;s screenplay tangle the puppeteer&#8217;s strings and play with narrative and filmmaking forces few are daring enough&#44; and smart enough&#44; to wield&#46;\n","                        Deep Focus Review                                                                                                                     Both cerebral and achingly emotional, Arrival sustains a message about hope and understanding for a better humanity that audiences may need right now.\n","                              Stuff.co.nz                                                                                                                                                                                One of the most beautiful, emotional and original science-fiction films of the last decade.\n","                              Film Frenzy                                                               The best film of 2016. A motion picture that turns out to be far more focused on humanity than on otherworldly visitors, it's a transcendent viewing experience that gets under the skin and into the heart.\n","                             Ars Technica                                                                                               Arrival just might be the overall best movie of the decade with the way it cleverly examines what we believe while simultaneously delivering a tense, gripping sci-fi story.\n","                       My New Plaid Pants                                                                                                                                       Hypnotic and strange and beautiful - have I said beautiful fifteen times yet? Because it deserves that word no fewer than that many.\n","                                      NME                                                                                                                                                                                                            An astonishingly beautiful film, both visually and emotionally.\n","   Charlotte Sometimes Goes to the Movies                                                                                                                                                                                           One of the finest, if not the finest, science fiction movie of the 21st Century.\n","                                Firstpost                                                                                                                                                                                       True to its theme, Arrival itself seems like a premonition of a more tangible event.\n","                             VultureHound                                                                                                                                                                                       A slow-burn thriller that is profound and beautiful. To say any more would spoil it,\n","                    Battleship Pretension                                                                                                                              We'll always have power structures that are reflexively defensive, but Arrival reminds us many in prominent positions are committed to peace.\n","                      Gone With The Twins                                                                                                                                                                                If there was such a precedent, this film might be considered an alien encounter procedural.\n","                           Film Companion                                                                                                                             Arrival expertly uses conventions of the genre to explore free-will, experiences and memory, in a film that is as personal as it is relatable.\n","                 TheIndependentCritic.com                                                                                                           A film as comfortable with universe-twisting ideologies as it is with the intimate, perhaps universal, experiences of communication, grief, discovery and trust.\n","                 World Socialist Web Site                                                                                                                                                                                                         Arrival is a feeble science fiction parable from Denis Villeneuve.\n","             Confessions From A Geek Mind                                                                                                                                                                                                            Arrival showcases the best performance you'll see by Amy Adams.\n","                                  Pólvora                                                                                                                    It's not easy to encounter a film that makes you meditate on its content days after watching it. And that's what Arrival does. [Full review in Spanish]\n","                         Cinemaficionados                                                                          Villeneuve's subtlety captures the close encounter of the third kind with dazzling imagery to reveal the contradictions of human existence. It is an amazing experience. [Full review in Spanish]\n","                            Movie Bitches                                                                                                                                                                                                                                               It's a very immersive movie.\n","                            Movie Bitches                                                                                                                                                                                                                                                       It was a cool movie.\n","                       Third Coast Review                                                                                                                                                   What I cherish so much about Arrival is that it places the highest value on science and smarts over braun and firepower.\n","The Magazine of Fantasy & Science Fiction                                                                     [The] movie-one of the best and brainiest sf films in recent memory-depicts the linguistic and scientific challenge of human scholars to understand and be understood by creatures from another world.\n","                            About Boulder                                                              The success of Arrival ultimately relies on the performance of Amy Adams. If she chose to retire from acting right now, she could be secure in the knowledge that her portrayal of Banks is career best work.\n","                   Sarah's Backstage Pass                                                              Arrival has aliens in it, but it's not a movie about aliens. It will challenge you with its complex plot and its ideas about life, death, communication and the nature of time. It is very much worth seeing.\n","                            Memphis Flyer                                           Villeneuve has accomplished what I thought impossible. Arrival stays true to the revelatory spirit of \"Story of Your Life\" while excising some of the story's more difficult concepts and adding a dash of Hollywood razzmatazz.\n"]}]},{"cell_type":"markdown","metadata":{"id":"ijRvPPgbjXv8"},"source":["4) Print the column names and number of rows/columns in the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733280275864,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"leqjKg-jPH8U","outputId":"84d8721b-bbbc-4bde-8318-c39d19912f0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Rotten Tomato Columns:\n","['id',\n"," 'reviewId',\n"," 'creationDate',\n"," 'criticName',\n"," 'isTopCritic',\n"," 'originalScore',\n"," 'reviewState',\n"," 'publicatioName',\n"," 'reviewText',\n"," 'scoreSentiment',\n"," 'reviewUrl']\n"]}],"source":["# Printing the RT_df column names with formatting\n","print(\"Rotten Tomato Columns:\")\n","pprint(list(RT_df.columns))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733280275864,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"EOsAjqR1rMgx","outputId":"344519da-f691-4087-a73b-b6cb9f0fb268"},"outputs":[{"output_type":"stream","name":"stdout","text":["The DataFrame has 294679 rows and 11 columns.\n"]}],"source":["# Printing the RT_df number of rows/columns with formatting\n","rows, cols = RT_df.shape\n","print(f\"The DataFrame has {rows} rows and {cols} columns.\")"]},{"cell_type":"markdown","metadata":{"id":"AdavMInycEv9"},"source":["## EDA\n","\n","1) What time frame is represented in this dataset?"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198,"status":"ok","timestamp":1733280276060,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"8CW1JQ-zsA6O","outputId":"cbfc246d-82f9-42c8-ad60-fae77899d71d"},"outputs":[{"output_type":"stream","name":"stdout","text":["reviewState\n","fresh     219628\n","rotten     75051\n","Name: count, dtype: int64\n"]}],"source":["# What are the review states/is there a reviewed state?\n","print(RT_df['reviewState'].value_counts()) # All completed reviews"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1733280276271,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"h4Tkh3K-cu-_","outputId":"7824c0f5-2e5d-4366-e1f7-fd30f5f6f6be"},"outputs":[{"output_type":"stream","name":"stdout","text":["First Review Time: 2020-01-02\n","Last Review Time: 2023-04-08\n","Total Time Range: 1192 days\n"]}],"source":["# Converting to time format\n","RT_df['creationDate'] = pd.to_datetime(RT_df['creationDate'])\n","\n","# Generating first review, last review, and total time range\n","# First review (earliest creationDate)\n","first_review_time = RT_df['creationDate'].min().date()\n","# Last review (latest creationDate)\n","last_review_time = RT_df['creationDate'].max().date()\n","\n","time_range = (last_review_time - first_review_time).days\n","\n","print(f\"First Review Time: {first_review_time}\")\n","print(f\"Last Review Time: {last_review_time}\")\n","print(f\"Total Time Range: {time_range} days\")"]},{"cell_type":"markdown","metadata":{"id":"UOHPsZKMcv4N"},"source":["2) How many top critics are there?"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733280276271,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"0nvFQsuTysKW","outputId":"a44bac0f-cb6e-43d9-d7da-ef7e785c9a3a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["isTopCritic\n","False    233965\n","True      60714\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>isTopCritic</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>False</th>\n","      <td>233965</td>\n","    </tr>\n","    <tr>\n","      <th>True</th>\n","      <td>60714</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":7}],"source":["# What are the itTopCritic Values\n","RT_df['isTopCritic'].value_counts()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1733280276271,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"Kc1TT7Jgc0Pi","outputId":"368038c4-50b1-4c66-b3ec-68262062277c"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 60714 top critics\n"]}],"source":["# How Many Top Critics are there\n","print(f\"There are {RT_df['isTopCritic'].sum()} top critics\")"]},{"cell_type":"markdown","metadata":{"id":"V2bDUvSIc6T7"},"source":["3) Do top Critics rate more favorably than regular critics?  Hint: Use `scoreSentiment` to answer"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dSlN6Mde39NB","executionInfo":{"status":"ok","timestamp":1733280276271,"user_tz":480,"elapsed":3,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}}},"outputs":[],"source":["def confidence_interval(data, confidence_level=0.95):\n","    # Calculate sample mean, standard deviation, and sample size\n","    mean = np.mean(data)\n","    std = np.std(data, ddof=1)  # Sample standard deviation\n","    n = len(data)\n","\n","    # Calculate the margin of error\n","    # Using a PPF for a point % vs. cumulative distribution < or >\n","    z_score = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n","    margin_of_error = z_score * (std / np.sqrt(n))\n","\n","    # Calculate confidence interval\n","    lower_bound = mean - margin_of_error\n","    upper_bound = mean + margin_of_error\n","\n","    return lower_bound, upper_bound, mean, margin_of_error\n","\n","# Need to turn sentiment into numerical encoding\n","sentiment_mapping = {\n","    'POSITIVE': 1,\n","    'NEGATIVE': -1\n","}\n","\n","RT_df['scoreSentimentNumeric'] = RT_df['scoreSentiment'].map(sentiment_mapping).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ssc6RdHnEpl8"},"outputs":[],"source":["# Filter the data for top critics and regular critics\n","top_critics = RT_df[RT_df['isTopCritic'] == True]['scoreSentimentNumeric']\n","regular_critics = RT_df[RT_df['isTopCritic'] == False]['scoreSentimentNumeric']\n","\n","top_critics_ci_lower, top_critics_ci_upper, top_critics_mean, top_critics_ME = confidence_interval(top_critics)\n","regular_critics_ci_lower, regular_critics_ci_upper, regular_critics_mean, regular_critics_ME = confidence_interval(regular_critics)\n","\n","print(f\"Top Critics - Mean sentiment: {top_critics_mean:.2f}, CI: ({top_critics_ci_lower:.3f}, {top_critics_ci_upper:.3f})\")\n","print(f\"Regular Critics - Mean sentiment: {regular_critics_mean:.2f}, CI: ({regular_critics_ci_lower:.3f}, {regular_critics_ci_upper:.3f})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSRIk-Oc5XWF"},"outputs":[],"source":["# Using the Confidence Interval to make a more attuned reccomendation\n","# Compare the confidence intervals\n","if top_critics_ci_upper < regular_critics_ci_lower:\n","    print(\"Regular critics rate more favorably (no overlap in CI).\")\n","elif regular_critics_ci_upper < top_critics_ci_lower:\n","    print(\"Top critics rate more favorably (no overlap in CI).\")\n","else:\n","    print(\"There is overlap in the confidence intervals, so we cannot \"\n","    \"confidently say whether top or regular critics rate more favorably.\")"]},{"cell_type":"markdown","metadata":{"id":"FZvBOMv5qG7k"},"source":["4) (Not required) Is there anything else you want to explore? Feel free to add more cells as needed!"]},{"cell_type":"markdown","metadata":{"id":"H-AbXIUPfjad"},"source":["## Visualization\n","\n","1. Visualize the top 10 publications by number of reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdtb2s6qEpl8"},"outputs":[],"source":["# Getting the top 10 publications ordered by # Reviews\n","publication_counts = RT_df['publicatioName'].value_counts().head(10)\n","publication_counts = publication_counts.round().astype(int)\n","publication_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFF8GjXpGXKU"},"outputs":[],"source":["# Setting up the visualizations\n","plt.figure(figsize=(10, 6))\n","sns.set(style=\"whitegrid\")\n","# Create a bar plot\n","ax = sns.barplot(x=publication_counts.values, y=publication_counts.index, palette=\"coolwarm\")\n","\n","plt.title('Top 10 Publications by Number of Reviews', fontsize=16, weight='bold', color='navy') # Navy color to match coolwarm\n","plt.xlabel('Number of Reviews', fontsize=14, weight='bold', color='darkslategray')\n","plt.ylabel('Publication Name', fontsize=14, weight='bold', color='darkslategray')\n","\n","# Labels on each bar\n","for p in ax.patches:\n","    # .get_height() / 2 to place label in middle of bar\n","    # .get_width() used to get right edge of each bar\n","    ax.annotate(f'{int(p.get_width())}', (p.get_width() + 0.2, p.get_y() + p.get_height() / 2),\n","                ha='center', va='center', fontsize=12, color='black')\n","\n","plt.tight_layout() # Avoids overlapping margins and white space\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LVW2d8TGqagm"},"source":["2. (Not required) Is there anything else you would like to visulalize? Feel free to add more cells as needed!"]},{"cell_type":"markdown","metadata":{"id":"_LCw1EvrZADO"},"source":["## Sweetviz Overview\n","Sweetviz is a fantastic tool for accelerating the EDA process by providing rich, intuitive visualizations, helping data scientists and analysts understand their datasets more effectively.\n","\n","It can also compare two datasets to see differences quickly for intricate drift analysis. Supports addition of a target variable as well.\n","\n","## Takeaway from the Sweetviz Report\n","**Multiple Rating Systems in the 'Original Score' Column**:\n","\n"," - The **'Original Score'** column appears to contain different reveiw scales, indicating a need for **standardization**. This is worthy of additional exploration.\n","\n"," - The **'Original Score'** column is also missing a high-degree of data (39%)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fg5IqNjgqazF"},"outputs":[],"source":["# Sweet Visualizer Report - addition of target of scoreSentiment\n","report = sv.analyze(RT_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cSj4-dFWymb"},"outputs":[],"source":["# Generate HTML report\n","report.show_html('RT_df_report.html')  # Save report as HTML file\n","\n","# Use ipython functions to visualize in notebook\n","display(HTML('RT_df_report.html'))  # Sweetviz .show_notbook() might be possible"]},{"cell_type":"markdown","metadata":{"id":"9IN8P7qaNZqI"},"source":["## NLP"]},{"cell_type":"markdown","metadata":{"id":"8RLYaSX8kRFF"},"source":["# Text Preprocessing Pipeline\n","\n","### Pre-Language Detection Text Cleaning Overview\n","\n","1. **HTML Encoding Removal**:\n","   - The function decodes HTML entities (e.g., `&#46;` becomes `.`) to ensure that the review text is in a readable format for all downstream steps\n","   \n","2. **Non-ASCII Character Removal**:\n","   - It removes non-ASCII characters (such as hidden or special whitespace characters) to avoid issues with unexpected or non-standard characters.\n","   \n","3. **New Line Character Handling**:\n","   - Multiple new lines are replaced with a single space to prevent fragmented sentences and maintain the text flow.\n","\n","4. **Lower Case, Contraction Expansion**\n","   - Tested as increasing performance of langdetect\n","\n","5. **Bracket Removal**\n","   - This is normally done for spelling correction in journalism\n","\n","### Confidence Score for English Classification in `langdetect`\n","\n","When using the **langdetect** library to classify whether a review is in English, a **confidence score** is returned along with the detected language. This score indicates the level of certainty the language detection algorithm has about the classification. Lang Detect is used to remove stop words in the appropriate language (english, french, and spanish supported)\n","\n","5.  **Stop Word Removal**\n","    - Stop word removal is done across **English, Spanish, and French**\n","\n","6. **Lemmatization**\n","    - This will convert words to their base form. This increases success of bi-gram calculation and sentiment analysis as it preserves meaning (e.g. 'Man is running' and 'Man ran' are the same)\n","    - Considered retaining ? and ! for sentiment analysis, but will do future improvement\n","\n","7. **Original Score Clean-up**\n","    - Handling a variety of diverse scores and cleaning them up into a 0-1 scale for downstream regression\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVKxniu-0q4X"},"outputs":[],"source":["# Function to clean the text: designed for preprocessing to language detection\n","def clean_text_lang(text):\n","    if isinstance(text, str):\n","        # Convert HTML entities to their corresponding characters\n","        text = html.unescape(text)\n","\n","        # Expand contractions\n","        text = contractions.fix(text)\n","\n","        # Convert text to lowercase\n","        # Tested as appropriate with langdetect and improves performance.\n","        text = text.lower()\n","\n","        # Remove any non-ASCII characters (e.g., special characters, zero-width spaces)\n","        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n","\n","        # Remove newline characters and carriage returns\n","        text = re.sub(r'\\n+', ' ', text)  # Replaces newlines with space\n","        text = re.sub(r'\\r+', ' ', text)  # Removes carriage returns\n","\n","        # Remove square brackets but keep anything between them - this is usually a spell correction\n","        text = re.sub(r'\\[|\\]', '', text)  # Removes '[' and ']'\n","\n","        # Clean up extra whitespace\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        return text\n","    else:\n","        return ''  # Return an empty string for non-string values\n","\n","# Get lang_code and is_english features by first cleaning the text\n","RT_df['cleaned_reviewText'] = RT_df['reviewText'].apply(lambda x: clean_text_lang(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11ERBBIJkby9"},"outputs":[],"source":["# Function to detect the language of the review text optimized for large volumes\n","def detect_language(text):\n","    try:\n","        # Detect the language: dictionary with 'lang' and 'score'\n","        result = detect(text)\n","        lang = result['lang']\n","        score = result['score']\n","        is_english = lang == 'en'  # Check if the language is English\n","        return lang, score, is_english\n","    except LangDetectException as e:  # Handle langdetect-specific exceptions\n","        print(f\"Error detecting language for text: {text[:100]}...\")\n","        return None, 0, False\n","\n","RT_df[['lang_code','lang_score','is_english']] = RT_df['cleaned_reviewText'].apply(lambda x: pd.Series(detect_language(x)))\n","\n","non_english_rows = RT_df[RT_df['is_english'] == False]\n","\n","# Checking total number of english detected rows\n","RT_df['is_english'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tey3UfYeHeSy"},"outputs":[],"source":["# Want only high confidence classifcation of FR and ES\n","RT_df['final_lang'] = RT_df.apply(\n","    lambda row: 'es' if row['lang_score'] > 0.8 and row['lang_code'] == 'es' else\n","                ('fr' if row['lang_score'] > 0.8 and row['lang_code'] == 'fr' else 'en'),\n","    axis=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"rCN_BoxOM0RJ"},"source":["### Text Cleaning and English Classification Check\n","\n","Below is a scrollable table that displays the original review text, the cleaned review text, and whether the review is in English or not. This allows for easy inspection of how the reviews were cleaned and if the language detection was accurate.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zo9U3eDZtJEU"},"outputs":[],"source":["# Small number of spanish review and french reviews\n","for index, row in non_english_rows.iterrows():\n","    print(f\"Lang Code: {row['lang_code']}\")\n","    print(f\"Lang Score: {row['lang_score']}\")\n","    print(f\"Is English: {row['is_english']}\")\n","    print(f\"Review Text: {row['reviewText']}\")\n","    print(f\"Cleaned Text: {row['cleaned_reviewText']}\")\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator between reviews"]},{"cell_type":"markdown","metadata":{"id":"EFXNw9AeN3VR"},"source":["1) Remove all stop words from `reviewText`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0_Wc0TtcKkw"},"outputs":[],"source":["# Load stopword lists for English, Spanish, and French\n","english_stopwords = set(stopwords.words('english'))\n","spanish_stopwords = set(stopwords.words('spanish'))\n","french_stopwords = set(stopwords.words('french'))\n","all_stopwords = english_stopwords.union(spanish_stopwords, french_stopwords)\n","\n","# Function to expand contractions (It's) and stop words\n","def remove_stopwords(text):\n","    # Tokenize the text into words\n","    words = word_tokenize(text)\n","\n","    # Remove stopwords from the list\n","    filtered_words = [word for word in words if word.lower() not in all_stopwords]\n","\n","    # Reassemble the text from the filtered words\n","    return ' '.join(filtered_words)\n","\n","RT_df['cleaned_reviewText_no_stopwords'] = RT_df['cleaned_reviewText'].apply(remove_stopwords)"]},{"cell_type":"markdown","metadata":{"id":"d1tJIJrQeODn"},"source":["### English Stop Word Removal Check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzaFLQ7aMeWD"},"outputs":[],"source":["# Unit testing (randomized) for appropriate stop-code removal\n","# 5 most common stopwords\n","stop_words = ['the', 'is', 'in', 'and', 'to']\n","\n","# Filter rows where at least one stop word is present in the original review text\n","filtered_rows = RT_df[RT_df['cleaned_reviewText'].apply(lambda x: any(word in x.lower() for word in stop_words))]\n","\n","# Randomly select 10 rows from the filtered rows\n","random_filtered_rows = filtered_rows.sample(n=10, random_state=42)  # You can adjust the number of rows as needed\n","\n","# Loop through the randomly selected rows and display the results\n","for index, row in random_filtered_rows.iterrows():\n","    print(f\"Original Review Text: {row['reviewText']}\")\n","    print(f\"Cleaned Review Text: {row['cleaned_reviewText']}\")\n","    print(f\"Cleaned Review Text (No Stopwords): {row['cleaned_reviewText_no_stopwords']}\")\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator between reviews"]},{"cell_type":"markdown","metadata":{"id":"lb7YMKKeeH7t"},"source":["### Spanish Stop Word Removal Check\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNJdb0t-PWrq"},"outputs":[],"source":["# Filter the rows where lang_code is 'es' and lang_score is greater than 0.90\n","filtered_spanish_reviews = RT_df[(RT_df['lang_code'] == 'es') & (RT_df['lang_score'] > 0.90)]\n","\n","# Loop through the filtered rows and display the review texts and their language scores\n","for index, row in filtered_spanish_reviews.iterrows():\n","    print(f\"Original Review Text: {row['reviewText']}\")\n","    print(f\"Cleaned Review Text: {row['cleaned_reviewText']}\")\n","    print(f\"Cleaned Review Text (No Stopwords): {row['cleaned_reviewText_no_stopwords']}\")\n","    print(\"\\n\" + \"-\"*50 + \"\\n\")  # Separator between reviews"]},{"cell_type":"markdown","metadata":{"id":"gXt4dhHYEpl9"},"source":["2. What is the most common bigram in reviewText (after stopword removal)?"]},{"cell_type":"markdown","metadata":{"id":"bTzKQLPUJJgr"},"source":["Note: Lemmatization is a higher performance option for better n-gram and sentiment analysis, but could take many hours to run.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bakMd6vk6nre"},"outputs":[],"source":["# Initialize lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Function to remove punctuation, digits, and stop words\n","def remove_unwanted_tokens_with_pos(text):\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Perform POS tagging\n","    tagged_tokens = nltk.pos_tag(tokens)\n","\n","    # Keep only the tokens with useful POS tags (e.g., nouns, verbs, adjectives, adverbs)\n","    useful_pos_tags = {'NN', 'VB', 'JJ', 'RB', 'NNS', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}  # You can add more tags if needed\n","    #     'NN': 'Singular noun (e.g., dog)',        # Nouns in singular form\n","    #     'VB': 'Base form verb (e.g., run)',       # Verbs in base form\n","    #     'JJ': 'Adjective (e.g., big)',            # Adjectives describing nouns\n","    #     'RB': 'Adverb (e.g., quickly)',           # Adverbs modifying verbs, adjectives, or other adverbs\n","    #     'NNS': 'Plural noun (e.g., dogs)',       # Nouns in plural form\n","    #     'VBD': 'Past tense verb (e.g., ran)',     # Verbs in past tense\n","    #     'VBG': 'Gerund/present participle verb (e.g., running)',  # Verb form used for continuous actions\n","    #     'VBN': 'Past participle verb (e.g., eaten)',  # Verb form used in perfect tenses\n","    #     'VBP': 'Non-3rd person singular present verb (e.g., eat)',  # Present tense verb (except for third-person singular)\n","    #     'VBZ': '3rd person singular present verb (e.g., eats)'  # Present tense verb for third-person singular subjects\n","\n","    filtered_tokens = [\n","        token.lower() for token, tag in tagged_tokens\n","        if tag[:2] in useful_pos_tags\n","    ]\n","\n","    return filtered_tokens\n","\n","# Function to lemmatize a single document\n","def lemmatize_text(text):\n","    tokens = remove_unwanted_tokens_with_pos(text)  # Clean and filter tokens\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize each token\n","    return \" \".join(lemmatized_tokens)  # Return the lemmatized text as a string\n","\n","# Function to process a list of texts in parallel\n","def parallel_lemmatization(texts):\n","    with ProcessPoolExecutor() as executor:\n","        results = list(executor.map(lemmatize_text, texts))\n","    return results\n","\n","# Apply the parallel lemmatization to the 'cleaned_reviewText_no_stopwords' column\n","texts_to_process = RT_df['cleaned_reviewText_no_stopwords'].astype(str).tolist()  # Convert to list of strings\n","\n","lemmatized_reviews = parallel_lemmatization(texts_to_process)\n","\n","# Add the lemmatized text back to the DataFrame\n","RT_df['lemmatized_reviewText'] = lemmatized_reviews\n","\n","# Optionally, inspect the result\n","print(RT_df[['cleaned_reviewText_no_stopwords', 'lemmatized_reviewText']].head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTwhvihcNfwy"},"outputs":[],"source":["# Generate bigrams for the cleaned review text\n","def get_top_bigrams(text_series, top_n=10):\n","    # Generate bigrams for the cleaned review text\n","    bigrams = []\n","    for review in text_series:\n","        tokens = word_tokenize(review)\n","        review_bigrams = list(ngrams(tokens, 2))  # Generate bigrams\n","        bigrams.extend(review_bigrams)  # Add them to the list of all bigrams\n","\n","    # Count frequency of each bigram\n","    bigram_counts = Counter(bigrams)\n","\n","    # Get the top N most common bigrams\n","    top_bigrams = bigram_counts.most_common(top_n)\n","\n","    return top_bigrams\n","\n","top_10_bigrams = get_top_bigrams(RT_df['lemmatized_reviewText'], top_n=10)\n","\n","# Print the top 10 bigrams\n","print(\"Top 10 Most Common Bigrams:\")\n","for bigram, count in top_10_bigrams:\n","    print(f\"Bigram: {bigram}, Count: {count}\")"]},{"cell_type":"markdown","metadata":{"id":"n2bVA_BjKjKV"},"source":["### Top Bi-Gram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5FBqGmIwHvy"},"outputs":[],"source":["# Getting the top bi-gram\n","top_bigram, top_bigram_count = top_10_bigrams[0]\n","print(f\"Top Bi-Gram: {top_bigram}, Count: {top_bigram_count}\")"]},{"cell_type":"markdown","metadata":{"id":"gaFBaupwEpl9"},"source":["3. Normalize the `originalScore` column to prepare it for downstream regression modeling.\n","\n","    One option is the following, though you are free to use any other normalization procedure.\n","\n","    * Limit the data to only those `originalScore`s with the format `x/y` where `y > x`.\n","    * Perform the division in the remaining strings to get a number between 0 and 1."]},{"cell_type":"markdown","metadata":{"id":"8Ps5uxybVWxv"},"source":["### Overview of Cleaning:\n","- Handle Case of Letter-plus and Letter-minus conversion\n","- Handle Case of Conversion of 'OF', 'OUT OF' and 'OUT' to a fraction\n","- Remove unwanted characters (?, ' for now)\n","- Remove 'STARS' in prep for other transformations\n","- Handle European decimal Formatting ',' to '.'; discovered '.' to '/' typo during this\n","- Convert '\\' to '/' which can be a common issue between OS\n","- Map typed letters to numbers\n","\n","### Transformation Design:\n","- Handle raw numeric conversion. Assume 0<=4 is 0-4 scale, 4<=10 is 0-10 scale and 10<=100 0-100 scale. (0-5 not accounted)\n","- Do a fraction conversion with error catching\n","- Do a 0-4.0 letter grade GPA transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkBvl1-xEpl9"},"outputs":[],"source":["# NOTE: We assume a 5 is on a 0-10 scale in this code, 0-5 scale is nor currently supported\n","\n","# Word-Grade mapping\n","word_to_number = {\n","    'ZERO': 0, 'ONE': 1, 'TWO': 2, 'THREE': 3, 'FOUR': 4, 'FIVE': 5,\n","    'SIX': 6, 'SEVEN': 7, 'EIGHT': 8, 'NINE': 9, 'TEN': 10\n","}\n","\n","# Grade mapping (normalized scores)\n","grade_mapping = {\n","    'A+': 4.0, 'A': 4.0, 'A-': 3.7,\n","    'B+': 3.3, 'B': 3.0, 'B-': 2.7,\n","    'C+': 2.3, 'C': 2.0, 'C-': 1.7,\n","    'D+': 1.3, 'D': 1.0, 'D-': 0.7,\n","    'F': 0.0\n","}\n","\n","def normalize_score(score):\n","    test = 0\n","    try:\n","        # Step 1: Convert score to string, strip whitespace and handle uppercase\n","        score = str(score).upper().strip()\n","        # Ensure regex is all capital\n","\n","        # Step 2: Convert 'C-plus' to 'C+' and 'C-minus' to 'C-', 'B-plus' to 'B+', etc.\n","        score = re.sub(r'([A-F])-PLUS', r'\\1+', re.sub(r'([A-F])-MINUS', r'\\1-', score))\n","\n","        # Step 3: Convert 'X of Y' or 'X out of Y' to 'X/Y'\n","        score = re.sub(r'\\s*(OF|OUT OF|OUT)\\s*', '/', score)\n","\n","        # Step 4: Remove unwanted characters (e.g., '?')\n","        score = re.sub(r'[?]', '', score)  # Remove '?' characters\n","\n","        # Step 5: Remove 'stars' first, before processing\n","        score = re.sub(r'\\s*STARS?\\s*', '', score)  # Remove any spaces around 'stars'\n","\n","        # Step 6: Convert commas to decimals (e.g., '1,5' to '1.5')\n","        score = score.replace(',', '.')\n","\n","        # Step 7: Handle 'backslash' case (convert '\\' to '/')\n","        score = score.replace('\\\\', '/')\n","\n","        # Step 8: Handle '4.5.5' case (convert second dot to '/')\n","        if re.match(r'\\d+\\.\\d+\\.\\d+', score):  # e.g., '4.5.5'\n","            score = score.replace('.', '/', 1)  # Replace first dot with '/'\n","\n","        # Step 9: Remove unnecessary whitespace\n","        score = score.replace(' ', '')   # Remove spaces completely\n","\n","        # Step 10: Remove unwanted apostrophes (e.g., '7/1\\' to '7/1')\n","        score = score.replace(\"'\", \"\")\n","\n","        # Step 11: Handle written-out numbers (e.g., \"three\", \"three stars\")\n","        score_words = score.split()\n","        for i, word in enumerate(score_words):\n","            word_lower = word.lower()\n","            if word_lower in word_to_number:\n","                score_words[i] = str(word_to_number[word_lower])\n","\n","        # Rejoin the words back into a single string\n","        score = ' '.join(score_words)\n","\n","        # Step 12: Handle string numeric values like '9.0', '4' or '3.5'\n","        if isinstance(score, str):\n","            try:\n","                score_value = float(score)  # Try to convert\n","                if 0 <= score_value <= 4:\n","                    return score_value / 4.0  # Normalize out of 4\n","                elif 4 < score_value <= 10:\n","                    return score_value / 10.0  # Normalize out of 10\n","                elif 10 < score_value <= 100:\n","                    return score_value / 100.0  # Normalize out of 100\n","                else:\n","                    pass\n","            except ValueError:\n","                pass  # If conversion fails, proceed with other checks\n","\n","        # Step 13: Handle #/# format (e.g., '8/10' or '3/4')\n","        if isinstance(score, str) and '/' in score:\n","            numerator, denominator = score.split('/')\n","            try:\n","                numerator = float(numerator.strip())\n","                denominator = float(denominator.strip())\n","                if denominator > 0:\n","                    return numerator / denominator  # Normalize by dividing\n","                else:\n","                    pass\n","            except ValueError:\n","                pass\n","\n","        # Step 14: Handle GPA grades like 'A+', 'B-', etc.\n","        elif isinstance(score, str):\n","            # Handle special cases: 'F-' should be considered as 'F'\n","            if score.startswith('F'):\n","                score = 'F'\n","\n","            # Handle valid GPA grades\n","            if score in grade_mapping:\n","                value = grade_mapping[score]\n","                return value / 4.0  # Normalize out of 4\n","            else:\n","                return None\n","\n","        return None\n","\n","    except Exception as e:\n","        print(f\"Error: {e} for score: {score}\")\n","        return None\n","\n","# Apply the normalization\n","RT_df['normalizedScore'] = RT_df['originalScore'].apply(normalize_score)\n","\n","# Number of NA\n","na_count = RT_df['normalizedScore'].isna().sum()\n","print(f\"\\nNumber of NA values in 'normalizedScore': {na_count}\")\n","\n","print(RT_df[['originalScore', 'normalizedScore']].head())"]},{"cell_type":"markdown","metadata":{"id":"vi6cUGIaXx9l"},"source":["### HTML Scroller for Normalization Edge Cases Not Caught"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnGSjES9DJaI"},"outputs":[],"source":["# Filter rows where normalizedScore is NaN but originalScore is not\n","filtered_df = RT_df[RT_df['normalizedScore'].isna() & RT_df['originalScore'].notna()]\n","\n","# Format the normalizedScore to 3 decimal places\n","filtered_df['normalizedScore'] = filtered_df['normalizedScore'].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else 'NaN')\n","html_table = filtered_df[['originalScore', 'normalizedScore']].to_html(index=False)\n","\n","# Create a scrollable HTML div\n","scrollable_html = f\"\"\"\n","<div style=\"height: 300px; overflow-y: scroll;\">\n","    {html_table}\n","</div>\n","\"\"\"\n","\n","# Display the scrollable table\n","display(HTML(scrollable_html))"]},{"cell_type":"markdown","metadata":{"id":"FQovYhD4X5Vr"},"source":["### HTML Scroller for Difficult Normalization String Cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NL8BocDWd0s"},"outputs":[],"source":["# Filter rows where normalizedScore is NaN but originalScore is not NaN and originalScore doesn't contain '/'\n","filtered_df = RT_df[RT_df['normalizedScore'].notna() & RT_df['originalScore'].notna() & ~RT_df['originalScore'].str.contains('/', na=False)]\n","\n","# Format the normalizedScore to 3 decimal places\n","filtered_df['normalizedScore'] = filtered_df['normalizedScore'].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else 'NaN')\n","\n","# Convert the filtered dataframe to HTML\n","html_table = filtered_df[['originalScore', 'normalizedScore']].to_html(index=False)\n","\n","# Create a scrollable HTML div\n","scrollable_html = f\"\"\"\n","<div style=\"height: 300px; overflow-y: scroll;\">\n","    {html_table}\n","</div>\n","\"\"\"\n","\n","# Display the scrollable table\n","display(HTML(scrollable_html))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3eTJeffMZsjx"},"outputs":[],"source":["# Spot-check of C-plus case (we want C+ not C- conversion)\n","# Filter rows where originalScore is \"C-plus\"\n","filtered_df = RT_df[RT_df['originalScore'].str.upper() == 'C-PLUS']\n","\n","# Display the filtered DataFrame with 'originalScore' and 'normalizedScore'\n","filtered_df[['originalScore', 'normalizedScore']].head(5)"]},{"cell_type":"markdown","metadata":{"id":"AwY96sstNmTf"},"source":["## Modeling\n","\n","1) Using any model you're comfortable with, train a sentiment classification model with `reviewText` as the basis for the features, and `scoreSentiment` as the outcome. Report out on the model's performance.  This *does not* need to be an exhaustive hyperparameter tuning job."]},{"cell_type":"markdown","metadata":{"id":"e3sbqV3zZitt"},"source":["## FastText AI model (lightweight)\n","\n","### Overview\n","**FastText** is a fast and efficient text representation model developed by Facebook AI. It improves upon Word2Vec by representing words as **character n-grams**, which helps handle rare, misspelled, or out-of-vocabulary words. FastText is optimized for quick training and large datasets, making it ideal for real-time NLP tasks.\n","\n","### Key Features:\n","- **Character-level n-grams**: Better handles rare and unseen words. We train on Word n-grams instead.\n","- **Fast and Efficient**: Scalable for large datasets with minimal memory usage.\n","- **Text Classification**: Supports sentiment analysis, spam detection, and more.\n","\n","### Use Cases:\n","1. **Text Classification**: Ideal for sentiment analysis and topic categorization.\n","2. **Word Embeddings**: Generates high-quality embeddings for downstream NLP tasks.\n","3. **Unsupervised Learning**: Works with clustering or sentiment lexicons for unsupervised sentiment analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oK790p1dZcwq"},"outputs":[],"source":["# Prepare the data for FastText\n","# Converting back to 0/1 binary for classification\n","RT_df['scoreSentimentBinary'] = RT_df['scoreSentimentNumeric'].apply(lambda x: 1 if x == 1 else 0)\n","\n","# FastText requires each text to be labeled with '__label__<label>'\n","train_df, test_df = train_test_split(RT_df, test_size=0.2, random_state=42)\n","\n","def prepare_fasttext_input(df, text_column, label_column):\n","    return df.apply(lambda x: f\"__label__{x[label_column]} {x[text_column]}\", axis=1).tolist()\n","\n","train_data = prepare_fasttext_input(train_df, 'lemmatized_reviewText', 'scoreSentimentBinary')\n","test_data = prepare_fasttext_input(test_df, 'lemmatized_reviewText', 'scoreSentimentBinary')\n","\n","# Save to text files (FastText requires these files for training and testing)\n","with open('train.txt', 'w') as f:\n","    f.write(\"\\n\".join(train_data))\n","\n","with open('test.txt', 'w') as f:\n","    f.write(\"\\n\".join(test_data))\n","\n","# Define hyperparameters for optimization\n","lr_values = [0.05]\n","epoch_values = [50]\n","wordngrams_values = [2,3,5,6]\n","bucket_value = 200000\n","\n","# Function to evaluate the model on the test set using AUC\n","def evaluate_model(model, test_df):\n","    y_true = test_df['scoreSentimentBinary']\n","    y_pred_prob = [model.predict(text)[1][0] for text in test_df['lemmatized_reviewText']]  # Get predicted probabilities\n","\n","    # Calculate AUC score\n","    auc_score = roc_auc_score(y_true, y_pred_prob)\n","\n","    return auc_score\n","\n","# Grid Search for Hyperparameters\n","best_auc = 0\n","best_model = None\n","best_params = {}\n","\n","for lr in lr_values:\n","    for epoch in epoch_values:\n","        for wordngrams in wordngrams_values:\n","              # Train FastText model with the current set of hyperparameters\n","              model = fasttext.train_supervised('train.txt', lr=lr, epoch=epoch, wordNgrams=wordngrams, bucket=bucket_value)\n","\n","              # Evaluate model performance using AUC\n","              auc_score = evaluate_model(model, test_df)\n","\n","              # If this is the best AUC score, save the model and parameters\n","              if auc_score > best_auc:\n","                  best_auc = auc_score\n","                  best_model = model\n","                  best_params = {'lr': lr, 'epoch': epoch, 'wordNgrams': wordngrams, 'bucket': bucket_value}\n","\n","print(\"Best Hyperparameters:\", best_params)\n","print(\"Best AUC Score on Test Set:\", best_auc)\n","\n","# Optionally, save the best model for later use\n","best_model.save_model(\"best_fasttext_model.bin\")"]},{"cell_type":"markdown","metadata":{"id":"us0ZkKYqvU7p"},"source":["Model shows more room to grow in it's hyperparameter optimization: What is the optimal word n-gram?\n","\n","Chose AUC as optimization metric as determining Beta of F1 can be difficult. Training on F1 was not successful resulting in a redundant threshold of .5 and 294676 : 3 Positive : Negative Sentiment Classification.\n","\n","Perform threshold optimization with F0.5, F1, and F2 instead."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5W1Pc6UbW7R"},"outputs":[],"source":["# best_model = fasttext.load_model(\"best_fasttext_model.bin\")\n","\n","def plot_pr_curve(precision, recall, thresholds):\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(recall, precision, color='b', label='PR Curve')\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Precision-Recall Curve')\n","    plt.legend(loc='best')\n","    plt.grid(True)\n","    plt.show()\n","\n","# Function to calculate Precision, Recall, F0.5, F1, and F2 at a threshold of 0.5\n","def evaluate_at_threshold(model, test_file, threshold=0.5):\n","    y_true = []\n","    y_pred = []\n","    y_scores = []\n","\n","    # Read the test file and make predictions\n","    with open(test_file, 'r') as f:\n","        for line in f:\n","            true_label = int(line.split()[0].replace('__label__', ''))\n","            text = \" \".join(line.split()[1:])\n","\n","            # Predict probabilities\n","            label, score = model.predict(text, k=-1)  # Get all labels and scores\n","            predicted_score = score[0]  # The confidence score for the predicted label\n","\n","            # Append true labels and predicted scores\n","            y_true.append(true_label)\n","            y_scores.append(predicted_score)\n","\n","            # Convert to binary prediction based on threshold\n","            predicted_label = 1 if predicted_score >= threshold else 0\n","            y_pred.append(predicted_label)\n","\n","    # Calculate precision, recall, F0.5 score, F1 score, and F2 score at threshold 0.5\n","    precision = precision_score(y_true, y_pred, pos_label=1)\n","    recall = recall_score(y_true, y_pred, pos_label=1)\n","    f0_5 = fbeta_score(y_true, y_pred, beta=0.5, pos_label=1)  # F0.5 score\n","    f1 = sklearn_f1_score(y_true, y_pred, pos_label=1)\n","    f2 = fbeta_score(y_true, y_pred, beta=2, pos_label=1)  # F2 score\n","\n","    # Calculate AUC score\n","    auc = roc_auc_score(y_true, y_scores)\n","\n","    print(f\"Precision at threshold {threshold}: {precision:.4f}\")\n","    print(f\"Recall at threshold {threshold}: {recall:.4f}\")\n","    print(f\"F0.5 Score at threshold {threshold}: {f0_5:.4f}\")\n","    print(f\"F1 Score at threshold {threshold}: {f1:.4f}\")\n","    print(f\"F2 Score at threshold {threshold}: {f2:.4f}\")\n","    print(f\"AUC Score at threshold {threshold}: {auc:.4f}\")\n","\n","    return y_true, y_pred, y_scores\n","\n","# Function for F0.5, F1, and F2 Threshold Optimization using sklearn\n","def optimize_thresholds(model, test_file):\n","    print(\"Evaluating at threshold 0.5:\")\n","    y_true, y_pred, y_scores = evaluate_at_threshold(model, test_file, threshold=0.5)\n","\n","    # PR curve for F1 score optimization\n","    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n","    f1_scores = 2 * (precision * recall) / (precision + recall)\n","\n","    # Calculate best F1 threshold\n","    best_f1_threshold = thresholds[np.argmax(f1_scores)]\n","    best_f1 = f1_scores[np.argmax(f1_scores)]\n","\n","    # Calculate best F0.5 threshold (more weight on Precision)\n","    f0_5_scores = (1 + 0.5**2) * (precision * recall) / ((0.5**2) * precision + recall)\n","    best_f0_5_threshold = thresholds[np.argmax(f0_5_scores)]\n","    best_f0_5 = f0_5_scores[np.argmax(f0_5_scores)]\n","\n","    # Calculate best F2 threshold (more weight on Recall)\n","    f2_scores = 2 * (precision * recall) / ((1 + 2**2) * (precision / 2 + recall))\n","    best_f2_threshold = thresholds[np.argmax(f2_scores)]\n","    best_f2 = f2_scores[np.argmax(f2_scores)]\n","\n","    # Re-evaluate at the best thresholds\n","    print(\"\\nAfter threshold optimization:\")\n","\n","    print(\"\\nF1 Threshold Optimization:\")\n","    _, _, _ = evaluate_at_threshold(model, test_file, threshold=best_f1_threshold)\n","\n","    print(\"\\nF0.5 Threshold Optimization:\")\n","    _, _, _ = evaluate_at_threshold(model, test_file, threshold=best_f0_5_threshold)\n","\n","    print(\"\\nF2 Threshold Optimization:\")\n","    _, _, _ = evaluate_at_threshold(model, test_file, threshold=best_f2_threshold)\n","\n","    print(f\"\\nOptimal threshold for F1 score: {best_f1_threshold:.4f}\")\n","    print(f\"Best F1 Score: {best_f1:.4f}\")\n","\n","    print(f\"Optimal threshold for F0.5 score: {best_f0_5_threshold:.4f}\")\n","    print(f\"Best F0.5 Score: {best_f0_5:.4f}\")\n","\n","    print(f\"Optimal threshold for F2 score: {best_f2_threshold:.4f}\")\n","    print(f\"Best F2 Score: {best_f2:.4f}\")\n","\n","    plot_pr_curve(precision, recall, thresholds)\n","\n","    return best_f1_threshold, best_f1, best_f0_5_threshold, best_f0_5, best_f2_threshold, best_f2\n","\n","# Evaluate and optimize F0.5, F1, and F2 scores\n","best_f1_threshold, best_f1, best_f0_5_threshold, best_f0_5, best_f2_threshold, best_f2 = optimize_thresholds(best_model, 'test.txt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmWEjlx6cvk1"},"outputs":[],"source":["# Change this to appropriate F1 metric\n","# Choosing F0.5 to preserve minority negative sentiment class\n","best_threshold = best_f0_5_threshold\n","\n","# Function to get predictions for the entire dataset\n","def predict_and_add_to_df(model, RT_df, threshold=0.5):\n","    predictions = []\n","\n","    for text in RT_df['lemmatized_reviewText']:\n","        # Predict label probabilities\n","        label, score = model.predict(text, k=-1)\n","        predicted_score = score[0]  # The confidence score for the predicted label\n","\n","        # Convert to binary prediction\n","        predicted_label = 1 if predicted_score >= threshold else -1\n","        predictions.append(predicted_label)\n","\n","    # Add the predictions as a new column to the DataFrame\n","    RT_df['predicted_sentiment'] = predictions\n","    return RT_df\n","\n","# Assuming `model` is the trained FastText model and RT_df contains the necessary column\n","RT_df = predict_and_add_to_df(best_model, RT_df, threshold=best_threshold)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1733112043325,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"},"user_tz":480},"id":"7pjOkco9dbAK","outputId":"4e4b4301-9975-4ab0-9b76-83d4904cd5d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Sentiment Distribution:\n","predicted_sentiment\n"," 1    287250\n","-1      7429\n","Name: count, dtype: int64\n"]}],"source":["print(\"Predicted Sentiment Distribution:\")\n","print(RT_df['predicted_sentiment'].value_counts())"]},{"cell_type":"markdown","metadata":{"id":"f3z334Fk-Prv"},"source":["## Conclusion\n","- Chose to optimize threshold on F0.5 to preserve minor class imbalance of Negative Sentiment\n","- Regardless of this optimization, AUC = .62 is weak. There is a lot of room for growth.\n","- Model is biased against negative sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAic-wvvBh9K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733112080353,"user_tz":480,"elapsed":37031,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"6f1faf57-396e-40df-b859-66c84ebae225"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","RT_df.to_pickle('/content/gdrive/My Drive/RT_df.pkl')"]},{"cell_type":"markdown","metadata":{"id":"Jv7l77Ygqnrn"},"source":["(Not required) Anything else you'd like to show us with this dataset / modeling effort?"]},{"cell_type":"markdown","metadata":{"id":"lPmRVzPUAbow"},"source":["## Pretrained Transformer Sentiment Model (Hugging Face)\n","Note this model will take a while to perform inference.\n","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment\n","\n","I would also explore the following two Transformers using adversarial training methodologies on IMDB datasets, which makes the model more resilient to real-world movie review noise which we witnessed in the Score Transformation.\n"," - https://huggingface.co/textattack/roberta-base-imdb\n"," - https://huggingface.co/textattack/bert-base-uncased-imdb/discussions\n"," - https://www.labellerr.com/blog/what-are-adversarial-attacks-in-machine-learning-and-how-can-you-prevent-them/\n","\n","<img src=\"https://www.labellerr.com/blog/content/images/2024/11/adversarial-attacks-machine-learning.webp\" alt=\"Adversarial Attacks in Machine Learning\" width=\"600\"/>"]},{"cell_type":"code","source":["# Bringing back in RT_df to notebook for Transformer\n","# NOTE: use GPU run-time\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","RT_df = pd.read_pickle('/content/gdrive/My Drive/RT_df.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctp05iL2hFQQ","executionInfo":{"status":"ok","timestamp":1733112361378,"user_tz":480,"elapsed":22583,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"905f6629-07d1-4092-edf0-054040fa7f6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKCllVZ6NnLW","outputId":"af07de46-723b-4c69-e082-f072e467f0c2","executionInfo":{"status":"ok","timestamp":1733119455019,"user_tz":480,"elapsed":2799778,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pretrained model and tokenizer...\n","Model loaded: BertConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"_name_or_path\": \"nlptown/bert-base-multilingual-uncased-sentiment\",\n","  \"_num_labels\": 5,\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": \"sentiment-analysis\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"1 star\",\n","    \"1\": \"2 stars\",\n","    \"2\": \"3 stars\",\n","    \"3\": \"4 stars\",\n","    \"4\": \"5 stars\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1 star\": 0,\n","    \"2 stars\": 1,\n","    \"3 stars\": 2,\n","    \"4 stars\": 3,\n","    \"5 stars\": 4\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.46.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 105879\n","}\n","\n","Applying sentiment analysis to reviews...\n","Transforming predicted sentiments to binary outcomes...\n"]}],"source":["# Step 1: Loading in a pretrained multilingual transformer\n","print(\"Loading pretrained model and tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n","model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n","\n","# Check if GPU is available and move the model to GPU if it is\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)  # Move the model to GPU if available, otherwise CPU\n","\n","# Set the model to evaluation mode (important for inference)\n","model.eval()\n","print(f\"Model loaded: {model.config}\")\n","\n","# Step 2: Tokenize and predict sentiment function\n","def sentiment_score(review):\n","    try:\n","        # Tokenizing the review to GPU\n","        tokens = tokenizer.encode(review, return_tensors='pt').to(device)\n","\n","        with torch.no_grad():  # No need to compute gradients for inference\n","            result = model(tokens)\n","\n","        # Extract the logits and calculate probabilities using softmax\n","        probabilities = torch.nn.functional.softmax(result.logits, dim=-1)\n","\n","        # Get the probability of the positive sentiment classes (sentiment 2, 3, and 4)\n","        # Class 2 is technically neutral\n","        # This step has to move tensors back into CPU, so process intensive\n","        positive_probabilities = probabilities.squeeze().cpu().numpy()[2:5].sum()\n","\n","        # Get the highest logit (index of the most likely sentiment class)\n","        sentiment = int(torch.argmax(result.logits))\n","\n","        probabilities_list = probabilities.squeeze().cpu().numpy().tolist()  # Convert tensor to list\n","\n","        return sentiment, positive_probabilities, probabilities_list\n","    except Exception as e:\n","        print(f\"Error during sentiment analysis: {e}\")\n","        return None, None, None\n","\n","# Step 3: Apply the sentiment_score function to the lemmatized review text\n","print(\"Applying sentiment analysis to reviews...\")\n","RT_df[['transformer_sentiment', 'transformer_positive_probabilities', 'transformer_predicted_probabilities']] = RT_df['lemmatized_reviewText'].apply(\n","    lambda review: pd.Series(sentiment_score(review))\n",")\n","\n","# Step 4: Convert scaled sentiment into binary outcome\n","def transform_predicted_sentiment(predicted):\n","    if predicted is None:  # Handle cases where the prediction might have failed\n","        return None\n","    elif predicted in [0, 1]:  # Sentiment 0 or 1 is negative\n","        return 0\n","    elif predicted in [3, 4]:  # Sentiment 3 or 4 is positive\n","        return 1\n","    else:  # output 2 (Neutral)\n","        return 1  # Treat Neutral (2) as positive\n","\n","print(\"Transforming predicted sentiments to binary outcomes...\")\n","RT_df['binary_transformer_sentiment'] = RT_df['transformer_sentiment'].apply(transform_predicted_sentiment)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLsM-SY3B9lG"},"outputs":[],"source":["# Saving off outcomes for analysis\n","RT_df.to_pickle('/content/gdrive/My Drive/RT_transformer_df.pkl')"]},{"cell_type":"code","source":["print(\"Transformer Predicted Sentiment Distribution:\")\n","print(RT_df['binary_transformer_sentiment'].value_counts())\n","print(RT_df['transformer_sentiment'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NczCqXcYwHUx","executionInfo":{"status":"ok","timestamp":1733119456828,"user_tz":480,"elapsed":5,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"b634c18d-97f8-4488-c8d2-d16ba5e84ba2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformer Predicted Sentiment Distribution:\n","binary_transformer_sentiment\n","1    199348\n","0     95331\n","Name: count, dtype: int64\n","transformer_sentiment\n","4    95696\n","3    67127\n","0    65905\n","2    36525\n","1    29426\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Prepare the true labels and predicted labels\n","y_true = RT_df['scoreSentimentNumeric'].apply(lambda x: 1 if x == 1 else 0)  # Convert 1/-1 to 1/0 for AUC calculation\n","y_pred = RT_df['binary_transformer_sentiment']  # Predicted sentiments from transformer\n","\n","# Calculate the probabilities for the positive class\n","y_prob = RT_df['transformer_positive_probabilities']  # Positive sentiment probabilities\n","\n","# Precision and Recall (binary classification, pos_label=1 for positive sentiment)\n","precision = precision_score(y_true, y_pred, pos_label=1)\n","recall = recall_score(y_true, y_pred, pos_label=1)\n","\n","# F1 Score (for positive class)\n","f1 = sklearn_f1_score(y_true, y_pred, pos_label=1)\n","\n","# F0.5 Score\n","beta = 0.5\n","f05 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n","\n","# AUC (Area Under the Curve)\n","auc = roc_auc_score(y_true, y_prob)\n","\n","# Print all metrics\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"F0.5 Score: {f05:.4f}\")\n","print(f\"AUC Score: {auc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNZ2FUrN4Oly","executionInfo":{"status":"ok","timestamp":1733119490298,"user_tz":480,"elapsed":994,"user":{"displayName":"Jordan Limperis","userId":"13371126362984529793"}},"outputId":"9051e964-7175-43b3-a467-d2eec5f37629"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.8424\n","Recall: 0.7646\n","F1 Score: 0.8016\n","F0.5 Score: 0.8256\n","AUC Score: 0.7542\n"]}]},{"cell_type":"markdown","source":["AUC Score: .7542"],"metadata":{"id":"isULCLcF_fU7"}}],"metadata":{"colab":{"provenance":[{"file_id":"1Wjbnhz-As4we4fufvYe-SAOM5JGAURTy","timestamp":1733119514825}],"gpuType":"T4"},"environment":{"name":"pytorch-gpu.1-9.m80","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m80"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}